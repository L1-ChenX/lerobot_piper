"""
lerobot_server_v3.py
ä¿®å¤ç‰ˆ: æ‰‹åŠ¨æ„é€ é…ç½® -> åŠ è½½ Base -> åŠ è½½ Adapter
"""
import zmq
import torch
import pickle
import numpy as np
import cv2
import json
from peft import PeftModel
from transformers import AutoTokenizer
from huggingface_hub import hf_hub_download

# å¼•å…¥ LeRobot çš„æ ¸å¿ƒç±»
from lerobot.policies.pi05.modeling_pi05 import PI05Policy
from lerobot.policies.pi05.configuration_pi05 import PI05Config
from lerobot.configs.types import PolicyFeature, FeatureType, NormalizationMode
from lerobot.utils.visualization_utils import init_rerun, log_rerun_data

# ================= é…ç½®åŒºåŸŸ =================
ADAPTER_REPO_ID = "moriis/pi05_piper_third"
BASE_MODEL_ID = "lerobot/pi05_base"
PORT = 5555
DEVICE = "cuda"
VISUALIZE = False

# [å…³é”®] è¿™é‡Œçš„ Key å¿…é¡»å’Œ config.json é‡Œçš„å®Œå…¨ä¸€è‡´
# å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ²¡æœ‰ Remap çš„é…ç½®
CAMERA_MAPPING = {
    "pikaGripperDepthCamera":   "observation.images.pikaGripperDepthCamera",
    "pikaGripperFisheyeCamera": "observation.images.pikaGripperFisheyeCamera",
    "pikaThirdPersonCamera":    "observation.images.pikaThirdPersonCamera",
}

MODEL_STATE_DIM = 32
MAX_TOKEN_LEN = 200
TOKENIZER_ID = "google/paligemma-3b-pt-224"
# ===========================================

def get_pi05_prompt(task_text, state_tensor):
    cleaned_text = task_text.strip().replace("_", " ").replace("\n", " ")
    state_np = state_tensor.cpu().numpy()
    state_np = np.clip(state_np, -1.0, 1.0)
    bins = np.linspace(-1, 1, 256 + 1)[:-1]
    discretized_states = np.digitize(state_np, bins) - 1
    state_str = " ".join(map(str, discretized_states))
    return f"Task: {cleaned_text}, State: {state_str};\nAction: "

def get_clean_config(repo_id):
    """
    æ‰‹åŠ¨ä¸‹è½½å¹¶æ„å»º PI05Config å¯¹è±¡ï¼Œç»•è¿‡æ‰€æœ‰è‡ªåŠ¨åŠ è½½çš„å‘
    """
    print(f"Downloading config from {repo_id}...")
    config_path = hf_hub_download(repo_id=repo_id, filename="config.json")
    
    with open(config_path, "r") as f:
        cfg_dict = json.load(f)

    # 1. æ¸…ç†ä¸æ”¯æŒçš„å­—æ®µ
    keys_to_remove = ["type", "transformers_version", "_commit_hash", "peft", "use_peft"]
    for k in keys_to_remove:
        if k in cfg_dict:
            del cfg_dict[k]

    # 2. è½¬æ¢ Enum ç±»å‹ (NormalizationMode)
    if "normalization_mapping" in cfg_dict:
        norm_map = {}
        for k, v in cfg_dict["normalization_mapping"].items():
            # å°†å­—ç¬¦ä¸² "IDENTITY" è½¬ä¸º NormalizationMode.IDENTITY
            norm_map[k] = NormalizationMode[v] if isinstance(v, str) else v
        cfg_dict["normalization_mapping"] = norm_map

    # 3. è½¬æ¢ PolicyFeature å¯¹è±¡
    def dict_to_feature(features_dict):
        new_features = {}
        for name, data in features_dict.items():
            if isinstance(data, dict):
                # å°† "VISUAL" å­—ç¬¦ä¸²è½¬ä¸º FeatureType.VISUAL
                if "type" in data and isinstance(data["type"], str):
                    data["type"] = FeatureType[data["type"]]
                new_features[name] = PolicyFeature(**data)
            else:
                new_features[name] = data
        return new_features

    if "input_features" in cfg_dict:
        cfg_dict["input_features"] = dict_to_feature(cfg_dict["input_features"])
    
    if "output_features" in cfg_dict:
        cfg_dict["output_features"] = dict_to_feature(cfg_dict["output_features"])

    # 4. å®ä¾‹åŒ–
    return PI05Config(**cfg_dict)

def main():
    print(f"Server A: Loading Tokenizer from {TOKENIZER_ID}...")
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_ID, padding_side="right")

    try:
        # [Step 1] è·å–å¹²å‡€çš„é…ç½®å¯¹è±¡
        user_config = get_clean_config(ADAPTER_REPO_ID)
        
        # [Step 2] åŠ è½½ Base æ¨¡å‹ï¼Œä½†å¼ºåˆ¶æ³¨å…¥æˆ‘ä»¬çš„ Config
        # è¿™ä¸€æ­¥ä¼šä¸‹è½½å¹¶åŠ è½½ lerobot/pi05_base çš„æƒé‡ï¼Œä½†ä½¿ç”¨ pika... çš„é…ç½®
        print(f"Server A: Loading BASE weights from {BASE_MODEL_ID} with CUSTOM CONFIG...")
        policy = PI05Policy.from_pretrained(BASE_MODEL_ID, config=user_config)
        
        # [Step 3] åŠ è½½ Adapter
        print(f"Server A: Loading Adapter from {ADAPTER_REPO_ID}...")
        policy = PeftModel.from_pretrained(policy, ADAPTER_REPO_ID)
        
        policy.to(DEVICE)
        policy.eval()
        print("âœ… Server A: Policy loaded successfully!")
        
    except Exception as e:
        print(f"âŒ Error loading policy: {e}")
        import traceback
        traceback.print_exc()
        return

    if VISUALIZE:
        init_rerun(session_name="Server_Inference", ip="127.0.0.1", port=9876)

    context = zmq.Context()
    socket = context.socket(zmq.REP)
    socket.bind(f"tcp://*:{PORT}")
    print(f"ğŸ§ Server A: Listening on port {PORT}...")
    
    while True:
        try:
            msg = socket.recv()
            payload = pickle.loads(msg)
            
            task_text = payload.get("text", "Grab the carrot")
            observation = {}
            processed_any_image = False

            # --- å›¾åƒå¤„ç† ---
            for hw_key, model_key in CAMERA_MAPPING.items():
                if hw_key in payload.get('images', {}):
                    img_bytes = payload['images'][hw_key]
                    if img_bytes is not None:
                        nparr = np.frombuffer(img_bytes, np.uint8)
                        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                        if frame is not None:
                            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                            frame_rgb = cv2.resize(frame_rgb, (224, 224))
                            observation[model_key] = torch.from_numpy(frame_rgb).permute(2, 0, 1).float() / 255.0
                            processed_any_image = True
            
            # å¡«å……ç¼ºå¤±å›¾åƒ (å¿…é¡»å¡«å……ï¼Œå¦åˆ™æ¨¡å‹ä¼šæŠ¥é”™)
            required_keys = list(CAMERA_MAPPING.values())
            if processed_any_image:
                for req_key in required_keys:
                    if req_key not in observation:
                        observation[req_key] = torch.zeros((3, 224, 224), dtype=torch.float32)

            # --- çŠ¶æ€å¤„ç† ---
            joints = payload.get('joint_state', [])
            gripper = payload.get('gripper_state', [0.0])
            base_state = list(joints) + list(gripper)
            
            if len(base_state) < MODEL_STATE_DIM:
                base_state += [0.0] * (MODEL_STATE_DIM - len(base_state))
            
            state_tensor = torch.tensor(base_state, dtype=torch.float32)
            observation["observation.state"] = state_tensor

            # --- æ¨ç† ---
            prompt = get_pi05_prompt(task_text, state_tensor)
            tokenized = tokenizer(prompt, return_tensors="pt", padding="max_length", max_length=MAX_TOKEN_LEN, truncation=True)

            batch = {k: v.unsqueeze(0).to(DEVICE) for k, v in observation.items() if isinstance(v, torch.Tensor)}
            batch["observation.language.tokens"] = tokenized.input_ids.to(DEVICE)
            batch["observation.language.attention_mask"] = tokenized.attention_mask.to(DEVICE).bool()

            with torch.no_grad():
                action = policy.select_action(batch)

            if action.ndim > 1: action = action[0]
            socket.send(pickle.dumps(action.cpu().numpy().tolist()[:13])) # åªè¿”å›å‰13ç»´

            if VISUALIZE:
                vis_obs = {k: v.cpu() for k, v in observation.items()}
                log_rerun_data(observation=vis_obs, action=action.cpu(), compress_images=False)

        except Exception as e:
            print(f"Error in loop: {e}")
            socket.send(pickle.dumps(None))

if __name__ == "__main__":
    main()